{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLCW_just_v3_algo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOh3IoapzwN81edXRNDc6sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiamFosterLFF/Bath_SWE_CW2/blob/main/RLCW_just_v3_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDTnzvDZcFod"
      },
      "source": [
        "# !git clone https://github.com/LiamFosterLFF/blockudoku.git\n",
        "# print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8uqLZZpcOGi"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "py_file_location = \"/\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import blockudoku as bd\n",
        "import blockudoku_toy_env as bdtoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgAvnliscSIs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "import pylab as plt\n",
        "# from google.colab import output\n",
        "import os.path\n",
        "from datetime import datetime\n",
        "\n",
        "class DQNAgent_with_ER_and_FTN_V3:\n",
        "  def __init__(self, verbose=2, state_includes_pieces=False, use_toy_env=False, clone=None):\n",
        "    # Parameters\n",
        "    self.epsilon = 0.99\n",
        "    self.epsilon_decay = .99\n",
        "    self.epsilon_min = .01\n",
        "    self.alpha = 0.15\n",
        "    self.gamma = 1\n",
        "    self.inverse_gamma = 1\n",
        "    self.d_min = 1000\n",
        "    self.d_max = 2000\n",
        "    self.c = 20\n",
        "    self.no_replays = 10\n",
        "\n",
        "    #Adjustable Parameters\n",
        "    self.verbose=verbose\n",
        "    self.start_time = datetime.now()\n",
        "    self.state_includes_pieces = state_includes_pieces;\n",
        "\n",
        "    self.env = bdtoy.SudokuTetrisGame() if use_toy_env else bd.SudokuTetrisGame()\n",
        "    \n",
        "    self.input_shape = (self.state_action_representation(self.env).size,)\n",
        "    \n",
        "    ## Algorithm: Deep Q-Learning with Experience Replay and Fixed Target Network\n",
        "    ## Initialise replay memory 𝐷 to capacity 𝑁\n",
        "    self.d = deque(maxlen = self.d_max)\n",
        "    ## Initialise action-value network 𝑞1 with random weights 𝜽1\n",
        "    self.q1 = self._build_DNN()\n",
        "    ## Initialise target action-value network 𝑞2 with weights 𝜽2 = 𝜽1\n",
        "    self.q2 = tf.keras.models.clone_model(self.q1)\n",
        "\n",
        "    if clone != None:\n",
        "      self.start_time = clone.start_time\n",
        "      self.d = clone.d\n",
        "      self.q1 = clone.q1\n",
        "      self.q2 = clone.q2\n",
        "\n",
        "  def train(self, episodes=1):\n",
        "    scores, eps = [], []\n",
        "    step_count = 1\n",
        "    ## For episode = 1, 𝑀 do\n",
        "    for e in range(episodes):\n",
        "      ## Initialise initial state 𝑆1\n",
        "      self.env.reset()\n",
        "      state = copy.deepcopy(self.env)\n",
        "      ## For 𝑡 = 1, 𝑇 do\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not done:\n",
        "        ## With probability 𝜖 select random action 𝐴𝑡\n",
        "        action = self.get_epsilon_greedy_action()\n",
        "        ## Execute action 𝐴𝑡 and observe reward 𝑅𝑡, next state 𝑆𝑡+1\n",
        "        _, reward, done, _ = self.env.step(action)\n",
        "        culm_reward=copy.deepcopy(reward)\n",
        "        if reward<5:\n",
        "          reward=-1\n",
        "        total_reward += culm_reward\n",
        "        ## Store transition (𝑆𝑡, 𝐴𝑡, 𝑅𝑡, 𝑆𝑡+1) in 𝐷\n",
        "        next_state = copy.deepcopy(self.env)\n",
        "        self.d.append((state, action, reward, next_state))\n",
        "\n",
        "        ## Sample random minibatch of transitions (𝑆𝑗, 𝐴𝑗, 𝑅𝑗, 𝑆𝑗+1) from 𝐷\n",
        "        if len(self.d) > self.d_min:\n",
        "          self.replay(self.no_replays)\n",
        "          \n",
        "        ## Every 𝐶 steps update 𝜽2 = 𝜽1\n",
        "        if step_count % self.c == 0:\n",
        "          self.q2 = tf.keras.models.clone_model(self.q1)\n",
        "        # update variables\n",
        "        step_count += 1\n",
        "        state = next_state\n",
        "\n",
        "        \n",
        "        if done:\n",
        "          ## Decay epsilon after each episode (not in original algorithm) (only if memory full)\n",
        "          if self.epsilon > self.epsilon_min and len(self.d) > self.d_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "          # every episode, plot the play time\n",
        "          eps.append(e+1)\n",
        "          scores.append(total_reward)\n",
        "          \n",
        "          # self.plot_print_and_write_to_file(eps, scores)\n",
        "          \n",
        "      ## End For\n",
        "    ## End For\n",
        "\n",
        "  def plot_print_and_write_to_file(self, eps, scores):\n",
        "    # Plot\n",
        "    x, y =  np.array(eps), np.array(scores)\n",
        "    \n",
        "    plt.plot(x, y, 'b')\n",
        "    m, b = np.polyfit(x, y, 1)\n",
        "    plt.plot(x, m*x + b, color='red')\n",
        "    plt.title('Total Reward Per Episode')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Total Reward')\n",
        "\n",
        "    # Print\n",
        "    if self.verbose >= 1:\n",
        "      plt_text = \"Episode # {} Final Score: {}, Memory Length: {}, Epsilon: {:.4f}, Slope: {:.4f}\".format(eps[-1], scores[-1], len(self.d), self.epsilon, m)\n",
        "      plt.text(.5, .05, plt_text)\n",
        "\n",
        "    # Write graph to file \"DQN_V3_output{datetime}.png\"\n",
        "    current_time = self.start_time.strftime(\"%a_%d%B%Y-%H:%M:%S\")\n",
        "    png_fname = \"DQN_V3_output-\" + current_time + \".png\"\n",
        "    plt.savefig('/content/{}'.format(png_fname))\n",
        "\n",
        "    # Save data to a csv-formatted file\n",
        "    csv_fname = \"'DQN_V3_data-\" + current_time + \".csv\"\n",
        "    if not os.path.exists(csv_fname):\n",
        "      with open(csv_fname, 'w+') as f:\n",
        "        title = \"episode_no,final_score,memory_length,epsilon,slope\\n\"\n",
        "        f.write(title)\n",
        "\n",
        "    with open(csv_fname, 'a') as f:\n",
        "      data = \"{},{},{},{},{}\\n\".format(eps[-1], scores[-1], len(self.d), self.epsilon, m)\n",
        "      f.write(data)\n",
        "\n",
        "    # # Save q1 and q2 to files (Use if Pickle doesn't work, otherwise delete)\n",
        "    # q1_model_fname = \"'DQN_V3_q1_model-\" + current_time\n",
        "    # tf.keras.models.save_model(self.q1, q1_model_fname, overwrite=True)\n",
        "    # q2_model_fname = \"'DQN_V3_q2_model-\" + current_time\n",
        "    # tf.keras.models.save_model(self.q1, q2_model_fname, overwrite=True)\n",
        "\n",
        "    if self.verbose == 2:\n",
        "      # Show plot\n",
        "      output.clear()\n",
        "      plt.show()\n",
        "\n",
        "  def get_greedy_action(self):\n",
        "    best_action = None\n",
        "    best_value = - float('inf')\n",
        "    for a in self.env.get_available_actions():\n",
        "      env_copy = copy.deepcopy(self.env)\n",
        "      _, reward, done, _ = env_copy.step(a)\n",
        "      if not done:\n",
        "        v = reward + self.q1.predict(self.state_action_representation(env_copy))[0][0]\n",
        "      else:\n",
        "        v = reward\n",
        "      if v > best_value:\n",
        "        best_action = a\n",
        "        best_value = v\n",
        "    action = best_action\n",
        "    return action\n",
        "\n",
        "  def get_greedy_action2(self):\n",
        "    done_arr, reward_arr, state_arr = [], [], []\n",
        "    action_arr = self.env.get_available_actions()\n",
        "    for a in action_arr:\n",
        "      env_copy = copy.deepcopy(self.env)\n",
        "      _, reward, done, _ = env_copy.step(a)\n",
        "      done_arr.append(0 if done else 1)\n",
        "      reward_arr.append(reward)\n",
        "      state_arr.append(self.state_action_representation(env_copy))\n",
        "    done_arr, reward_arr, state_arr = np.array(done_arr), np.array(reward_arr), np.array(state_arr)\n",
        "    # Run predict on a batch of all possible states for action\n",
        "    next_value_arr = np.reshape(self.q1.predict(np.array(state_arr)), reward_arr.shape)\n",
        "    # v = r + (v_next_state if done else 0)\n",
        "    v_arr = np.add(np.array(reward_arr), np.multiply(np.array(done_arr), next_value_arr))\n",
        "    # Choose best action from v then find corresponding action\n",
        "    action = action_arr[np.argmax(v_arr)]\n",
        "    return action\n",
        "    \n",
        "  def get_epsilon_greedy_action(self):\n",
        "    # Get Random Action\n",
        "    if self.epsilon > random.random():\n",
        "      return self.env.sample_action()\n",
        "    ## Greedy Action: With probability 1 − 𝜖\n",
        "    else:\n",
        "      # return self.get_greedy_action2()\n",
        "      return self.get_greedy_action()\n",
        "\n",
        "  def replay(self, no_replays):\n",
        "      for i in range(no_replays):\n",
        "        state, action, reward, next_state = random.choice(self.d)\n",
        "        yj = 0\n",
        "        ## if 𝑆𝑗+1 is terminal: Set 𝑦𝑗 = 𝑅𝑗 + 0\n",
        "        if next_state.check_if_done():\n",
        "          yj = reward        \n",
        "        ## otherwise: Set 𝑦𝑗 = 𝑅𝑗 + max 𝑎′𝑞2(𝑆𝑗+1, 𝑎′, 𝜽2)\n",
        "        else:\n",
        "          # prediction = self.predict_q2(next_state).flatten()\n",
        "          # best_action = np.nanargmax(prediction)\n",
        "          # best_value = prediction[best_action]\n",
        "          # yj[action] = self.inverse_gamma*reward + self.gamma*best_value\n",
        "          yj = reward + self.predict_q2(next_state)[0][0]\n",
        "        ## Perform gradient descent step ∇𝜽1𝐿𝛿(𝑦𝑗, 𝑞1(𝑆𝑗, 𝐴𝑗, 𝜽1))\n",
        "        self.q1.fit(self.state_action_representation(state), np.array([yj]).reshape(1, -1), epochs=1, verbose=False)\n",
        "\n",
        "  def _build_DNN(self):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=self.input_shape))\n",
        "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(1, activation='linear', kernel_initializer='he_uniform'))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(lr=self.alpha))\n",
        "    return model\n",
        "\n",
        "  def predict_q1(self, state) :\n",
        "    DNN_input = self.state_action_representation(state)\n",
        "    value_prediction = self.q1.predict(DNN_input)\n",
        "    return value_prediction\n",
        "\n",
        "  def predict_q2(self, state):\n",
        "    DNN_input = self.state_action_representation(state)\n",
        "    value_prediction = self.q2.predict(DNN_input)\n",
        "    return value_prediction\n",
        "\n",
        "  def state_action_representation(self, state):\n",
        "    # Get state representation\n",
        "    board_rep = [1 if square else 0 for square in state.board.flatten()]\n",
        "    # Change parameter to include the pieces in the state representation (Optional)\n",
        "    if self.state_includes_pieces:\n",
        "      pieces_rep = [1 if bd.PIECES[x] in state.current_pieces else 0 for x in range(len(bd.PIECES))]\n",
        "      board_rep = board_rep + pieces_rep\n",
        "\n",
        "    return np.array(board_rep).reshape(-1, len(board_rep))\n",
        "\n",
        "agent = DQNAgent_with_ER_and_FTN_V3()\n",
        "agent.train(10000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}