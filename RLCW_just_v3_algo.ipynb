{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RLCW_just_v3_algo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOh3IoapzwN81edXRNDc6sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiamFosterLFF/Bath_SWE_CW2/blob/main/RLCW_just_v3_algo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDTnzvDZcFod"
      },
      "source": [
        "# !git clone https://github.com/LiamFosterLFF/blockudoku.git\n",
        "# print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8uqLZZpcOGi"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "\n",
        "py_file_location = \"/\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import blockudoku as bd\n",
        "import blockudoku_toy_env as bdtoy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgAvnliscSIs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Dense, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "import pylab as plt\n",
        "# from google.colab import output\n",
        "import os.path\n",
        "from datetime import datetime\n",
        "\n",
        "class DQNAgent_with_ER_and_FTN_V3:\n",
        "  def __init__(self, verbose=2, state_includes_pieces=False, use_toy_env=False, clone=None):\n",
        "    # Parameters\n",
        "    self.epsilon = 0.99\n",
        "    self.epsilon_decay = .99\n",
        "    self.epsilon_min = .01\n",
        "    self.alpha = 0.15\n",
        "    self.gamma = 1\n",
        "    self.inverse_gamma = 1\n",
        "    self.d_min = 1000\n",
        "    self.d_max = 2000\n",
        "    self.c = 20\n",
        "    self.no_replays = 10\n",
        "\n",
        "    #Adjustable Parameters\n",
        "    self.verbose=verbose\n",
        "    self.start_time = datetime.now()\n",
        "    self.state_includes_pieces = state_includes_pieces;\n",
        "\n",
        "    self.env = bdtoy.SudokuTetrisGame() if use_toy_env else bd.SudokuTetrisGame()\n",
        "    \n",
        "    self.input_shape = (self.state_action_representation(self.env).size,)\n",
        "    \n",
        "    ## Algorithm: Deep Q-Learning with Experience Replay and Fixed Target Network\n",
        "    ## Initialise replay memory ğ· to capacity ğ‘\n",
        "    self.d = deque(maxlen = self.d_max)\n",
        "    ## Initialise action-value network ğ‘1 with random weights ğœ½1\n",
        "    self.q1 = self._build_DNN()\n",
        "    ## Initialise target action-value network ğ‘2 with weights ğœ½2 = ğœ½1\n",
        "    self.q2 = tf.keras.models.clone_model(self.q1)\n",
        "\n",
        "    if clone != None:\n",
        "      self.start_time = clone.start_time\n",
        "      self.d = clone.d\n",
        "      self.q1 = clone.q1\n",
        "      self.q2 = clone.q2\n",
        "\n",
        "  def train(self, episodes=1):\n",
        "    scores, eps = [], []\n",
        "    step_count = 1\n",
        "    ## For episode = 1, ğ‘€ do\n",
        "    for e in range(episodes):\n",
        "      ## Initialise initial state ğ‘†1\n",
        "      self.env.reset()\n",
        "      state = copy.deepcopy(self.env)\n",
        "      ## For ğ‘¡ = 1, ğ‘‡ do\n",
        "      done = False\n",
        "      total_reward = 0\n",
        "      while not done:\n",
        "        ## With probability ğœ– select random action ğ´ğ‘¡\n",
        "        action = self.get_epsilon_greedy_action()\n",
        "        ## Execute action ğ´ğ‘¡ and observe reward ğ‘…ğ‘¡, next state ğ‘†ğ‘¡+1\n",
        "        _, reward, done, _ = self.env.step(action)\n",
        "        culm_reward=copy.deepcopy(reward)\n",
        "        if reward<5:\n",
        "          reward=-1\n",
        "        total_reward += culm_reward\n",
        "        ## Store transition (ğ‘†ğ‘¡, ğ´ğ‘¡, ğ‘…ğ‘¡, ğ‘†ğ‘¡+1) in ğ·\n",
        "        next_state = copy.deepcopy(self.env)\n",
        "        self.d.append((state, action, reward, next_state))\n",
        "\n",
        "        ## Sample random minibatch of transitions (ğ‘†ğ‘—, ğ´ğ‘—, ğ‘…ğ‘—, ğ‘†ğ‘—+1) from ğ·\n",
        "        if len(self.d) > self.d_min:\n",
        "          self.replay(self.no_replays)\n",
        "          \n",
        "        ## Every ğ¶ steps update ğœ½2 = ğœ½1\n",
        "        if step_count % self.c == 0:\n",
        "          self.q2 = tf.keras.models.clone_model(self.q1)\n",
        "        # update variables\n",
        "        step_count += 1\n",
        "        state = next_state\n",
        "\n",
        "        \n",
        "        if done:\n",
        "          ## Decay epsilon after each episode (not in original algorithm) (only if memory full)\n",
        "          if self.epsilon > self.epsilon_min and len(self.d) > self.d_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "          # every episode, plot the play time\n",
        "          eps.append(e+1)\n",
        "          scores.append(total_reward)\n",
        "          \n",
        "          # self.plot_print_and_write_to_file(eps, scores)\n",
        "          \n",
        "      ## End For\n",
        "    ## End For\n",
        "\n",
        "  def plot_print_and_write_to_file(self, eps, scores):\n",
        "    # Plot\n",
        "    x, y =  np.array(eps), np.array(scores)\n",
        "    \n",
        "    plt.plot(x, y, 'b')\n",
        "    m, b = np.polyfit(x, y, 1)\n",
        "    plt.plot(x, m*x + b, color='red')\n",
        "    plt.title('Total Reward Per Episode')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Total Reward')\n",
        "\n",
        "    # Print\n",
        "    if self.verbose >= 1:\n",
        "      plt_text = \"Episode # {} Final Score: {}, Memory Length: {}, Epsilon: {:.4f}, Slope: {:.4f}\".format(eps[-1], scores[-1], len(self.d), self.epsilon, m)\n",
        "      plt.text(.5, .05, plt_text)\n",
        "\n",
        "    # Write graph to file \"DQN_V3_output{datetime}.png\"\n",
        "    current_time = self.start_time.strftime(\"%a_%d%B%Y-%H:%M:%S\")\n",
        "    png_fname = \"DQN_V3_output-\" + current_time + \".png\"\n",
        "    plt.savefig('/content/{}'.format(png_fname))\n",
        "\n",
        "    # Save data to a csv-formatted file\n",
        "    csv_fname = \"'DQN_V3_data-\" + current_time + \".csv\"\n",
        "    if not os.path.exists(csv_fname):\n",
        "      with open(csv_fname, 'w+') as f:\n",
        "        title = \"episode_no,final_score,memory_length,epsilon,slope\\n\"\n",
        "        f.write(title)\n",
        "\n",
        "    with open(csv_fname, 'a') as f:\n",
        "      data = \"{},{},{},{},{}\\n\".format(eps[-1], scores[-1], len(self.d), self.epsilon, m)\n",
        "      f.write(data)\n",
        "\n",
        "    # # Save q1 and q2 to files (Use if Pickle doesn't work, otherwise delete)\n",
        "    # q1_model_fname = \"'DQN_V3_q1_model-\" + current_time\n",
        "    # tf.keras.models.save_model(self.q1, q1_model_fname, overwrite=True)\n",
        "    # q2_model_fname = \"'DQN_V3_q2_model-\" + current_time\n",
        "    # tf.keras.models.save_model(self.q1, q2_model_fname, overwrite=True)\n",
        "\n",
        "    if self.verbose == 2:\n",
        "      # Show plot\n",
        "      output.clear()\n",
        "      plt.show()\n",
        "\n",
        "  def get_greedy_action(self):\n",
        "    best_action = None\n",
        "    best_value = - float('inf')\n",
        "    for a in self.env.get_available_actions():\n",
        "      env_copy = copy.deepcopy(self.env)\n",
        "      _, reward, done, _ = env_copy.step(a)\n",
        "      if not done:\n",
        "        v = reward + self.q1.predict(self.state_action_representation(env_copy))[0][0]\n",
        "      else:\n",
        "        v = reward\n",
        "      if v > best_value:\n",
        "        best_action = a\n",
        "        best_value = v\n",
        "    action = best_action\n",
        "    return action\n",
        "\n",
        "  def get_greedy_action2(self):\n",
        "    done_arr, reward_arr, state_arr = [], [], []\n",
        "    action_arr = self.env.get_available_actions()\n",
        "    for a in action_arr:\n",
        "      env_copy = copy.deepcopy(self.env)\n",
        "      _, reward, done, _ = env_copy.step(a)\n",
        "      done_arr.append(0 if done else 1)\n",
        "      reward_arr.append(reward)\n",
        "      state_arr.append(self.state_action_representation(env_copy))\n",
        "    done_arr, reward_arr, state_arr = np.array(done_arr), np.array(reward_arr), np.array(state_arr)\n",
        "    # Run predict on a batch of all possible states for action\n",
        "    next_value_arr = np.reshape(self.q1.predict(np.array(state_arr)), reward_arr.shape)\n",
        "    # v = r + (v_next_state if done else 0)\n",
        "    v_arr = np.add(np.array(reward_arr), np.multiply(np.array(done_arr), next_value_arr))\n",
        "    # Choose best action from v then find corresponding action\n",
        "    action = action_arr[np.argmax(v_arr)]\n",
        "    return action\n",
        "    \n",
        "  def get_epsilon_greedy_action(self):\n",
        "    # Get Random Action\n",
        "    if self.epsilon > random.random():\n",
        "      return self.env.sample_action()\n",
        "    ## Greedy Action: With probability 1 âˆ’ ğœ–\n",
        "    else:\n",
        "      # return self.get_greedy_action2()\n",
        "      return self.get_greedy_action()\n",
        "\n",
        "  def replay(self, no_replays):\n",
        "      for i in range(no_replays):\n",
        "        state, action, reward, next_state = random.choice(self.d)\n",
        "        yj = 0\n",
        "        ## if ğ‘†ğ‘—+1 is terminal: Set ğ‘¦ğ‘— = ğ‘…ğ‘— + 0\n",
        "        if next_state.check_if_done():\n",
        "          yj = reward        \n",
        "        ## otherwise: Set ğ‘¦ğ‘— = ğ‘…ğ‘— + max ğ‘â€²ğ‘2(ğ‘†ğ‘—+1, ğ‘â€², ğœ½2)\n",
        "        else:\n",
        "          # prediction = self.predict_q2(next_state).flatten()\n",
        "          # best_action = np.nanargmax(prediction)\n",
        "          # best_value = prediction[best_action]\n",
        "          # yj[action] = self.inverse_gamma*reward + self.gamma*best_value\n",
        "          yj = reward + self.predict_q2(next_state)[0][0]\n",
        "        ## Perform gradient descent step âˆ‡ğœ½1ğ¿ğ›¿(ğ‘¦ğ‘—, ğ‘1(ğ‘†ğ‘—, ğ´ğ‘—, ğœ½1))\n",
        "        self.q1.fit(self.state_action_representation(state), np.array([yj]).reshape(1, -1), epochs=1, verbose=False)\n",
        "\n",
        "  def _build_DNN(self):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=self.input_shape))\n",
        "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(1, activation='linear', kernel_initializer='he_uniform'))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(lr=self.alpha))\n",
        "    return model\n",
        "\n",
        "  def predict_q1(self, state) :\n",
        "    DNN_input = self.state_action_representation(state)\n",
        "    value_prediction = self.q1.predict(DNN_input)\n",
        "    return value_prediction\n",
        "\n",
        "  def predict_q2(self, state):\n",
        "    DNN_input = self.state_action_representation(state)\n",
        "    value_prediction = self.q2.predict(DNN_input)\n",
        "    return value_prediction\n",
        "\n",
        "  def state_action_representation(self, state):\n",
        "    # Get state representation\n",
        "    board_rep = [1 if square else 0 for square in state.board.flatten()]\n",
        "    # Change parameter to include the pieces in the state representation (Optional)\n",
        "    if self.state_includes_pieces:\n",
        "      pieces_rep = [1 if bd.PIECES[x] in state.current_pieces else 0 for x in range(len(bd.PIECES))]\n",
        "      board_rep = board_rep + pieces_rep\n",
        "\n",
        "    return np.array(board_rep).reshape(-1, len(board_rep))\n",
        "\n",
        "agent = DQNAgent_with_ER_and_FTN_V3()\n",
        "agent.train(10000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}